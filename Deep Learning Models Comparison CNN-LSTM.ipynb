{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>first think another disney movie might good it...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>put aside dr house repeat missed desperate hou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>big fan stephen king s work film made even gre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>watched horrid thing tv needless say one movie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>truly enjoyed film acting terrific plot jeff c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       SentimentText  Sentiment\n",
       "0  first think another disney movie might good it...          1\n",
       "1  put aside dr house repeat missed desperate hou...          0\n",
       "2  big fan stephen king s work film made even gre...          1\n",
       "3  watched horrid thing tv needless say one movie...          0\n",
       "4  truly enjoyed film acting terrific plot jeff c...          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "csv = 'dataset/cleand.csv'\n",
    "data = pd.read_csv(csv,index_col=0, encoding='latin-1')\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\it\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "SEED = 2000\n",
    "\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(data.SentimentText, data.Sentiment, test_size=.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next, we will use Keras Tokenizer to split each word in a sentence. Then, in order to get a sequential representation of each row we use texts_to_sequences method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\it\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "974"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = []\n",
    "for x in x_train:\n",
    "    length.append(len(x.split()))\n",
    "max(length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can say that the maximum lenght to be 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 1265,   16,    6],\n",
       "       [   0,    0,    0, ..., 1468,  213,  237],\n",
       "       [   0,    0,    0, ...,   58,  100,    9],\n",
       "       [   0,    0,    0, ...,   43,  664,  141],\n",
       "       [   0,    0,    0, ...,    5,    1,  169]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_seq = pad_sequences(sequences, maxlen=1000)\n",
    "x_train_seq[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After checking, we can see that all the data transformed to have the same length of 1000.\n",
    "\n",
    "We do the same thing to the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_val = tokenizer.texts_to_sequences(x_validation)\n",
    "x_val_seq = pad_sequences(sequences_val, maxlen=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      " - 376s - loss: 0.3832 - acc: 0.8166 - val_loss: 0.2775 - val_acc: 0.8832\n",
      "Epoch 2/5\n",
      " - 351s - loss: 0.1296 - acc: 0.9528 - val_loss: 0.3038 - val_acc: 0.8828\n",
      "Epoch 3/5\n",
      " - 352s - loss: 0.0161 - acc: 0.9966 - val_loss: 0.3731 - val_acc: 0.8836\n",
      "Epoch 4/5\n",
      " - 352s - loss: 0.0012 - acc: 1.0000 - val_loss: 0.4015 - val_acc: 0.8894\n",
      "Epoch 5/5\n",
      " - 351s - loss: 2.3939e-04 - acc: 1.0000 - val_loss: 0.4156 - val_acc: 0.8890\n",
      "score: 0.42\n",
      "acc: 0.89\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from time import time\n",
    "\n",
    "acc = []\n",
    "times = []\n",
    "\n",
    "model_cnn = Sequential()\n",
    "\n",
    "e = Embedding(100000, 100, input_length=1000)\n",
    "model_cnn.add(e)\n",
    "model_cnn.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "model_cnn.add(GlobalMaxPooling1D())\n",
    "model_cnn.add(Dense(256, activation='relu'))\n",
    "model_cnn.add(Dense(1, activation='sigmoid'))\n",
    "model_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "t0 = time()\n",
    "model_cnn.fit(x_train_seq, y_train, validation_data=(x_val_seq, y_validation), epochs=5, batch_size=32, verbose=2)\n",
    "score,accu = model_cnn.evaluate(x_val_seq, y_validation, verbose = 2, batch_size = 32)\n",
    "tv_time = time()-t0\n",
    "\n",
    "acc.append(accu*100)\n",
    "times.append(tv_time*0.0166667)\n",
    "\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (accu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short Term Memory\n",
    "Let's now try another model LSTM and compare it with the previous CNN model. We will use a single LSTM layer preceded by an embedding layer with 100000 as max feature and 128 dimension of each word in a sequence, then followed with a dense layer with softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('dataset/cleand.csv',delimiter=',',encoding='latin-1')\n",
    "#df.head()\n",
    "data = pd.read_csv('dataset/cleand.csv')\n",
    "# Keeping only the neccessary columns\n",
    "data = data[['SentimentText','Sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(data['SentimentText'].values)\n",
    "X = tokenizer.texts_to_sequences(data['SentimentText'].values)\n",
    "X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 741, 128)          256000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 741, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 394       \n",
      "=================================================================\n",
      "Total params: 511,194\n",
      "Trainable params: 511,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16750, 741) (16750, 2)\n",
      "(8250, 741) (8250, 2)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(data['Sentiment']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " - 1310s - loss: 0.4644 - acc: 0.7781\n",
      "Epoch 2/5\n",
      " - 1206s - loss: 0.3577 - acc: 0.8490\n",
      "Epoch 3/5\n",
      " - 1652s - loss: 0.3075 - acc: 0.8749\n",
      "Epoch 4/5\n",
      " - 2328s - loss: 0.2801 - acc: 0.8869\n",
      "Epoch 5/5\n",
      " - 2927s - loss: 0.2551 - acc: 0.8996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12456860>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs = 5, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.37\n",
      "acc: 0.85\n"
     ]
    }
   ],
   "source": [
    "validation_size = 1500\n",
    "\n",
    "X_validate = X_test[-validation_size:]\n",
    "Y_validate = Y_test[-validation_size:]\n",
    "X_test = X_test[:-validation_size]\n",
    "Y_test = Y_test[:-validation_size]\n",
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
